{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参照\n",
    "    - [RAGコンペ参加記 (raggle)](https://qiita.com/ctc-j-ikai/items/9980f6a1c11ef444ba4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-rohto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pdfplumber\n",
    "\n",
    "\n",
    "def load_pdf_document(file_path):\n",
    "    documents = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            documents.append({\"content\": page.extract_text(), \"metadata\": {\"source\": file_path}})\n",
    "    return documents\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    \"../dataset/Financial_Statements_2023.pdf\",\n",
    "    \"../dataset/Hada_Labo_Gokujun_Lotion_Overview.pdf\",\n",
    "    \"../dataset/Shibata_et_al_Research_Article.pdf\",\n",
    "    \"../dataset/V_Rohto_Premium_Product_Information.pdf\",\n",
    "    \"../dataset/Well-Being_Report_2024.pdf\",\n",
    "]\n",
    "\n",
    "# PDFファイルを読み込む\n",
    "docs = [load_pdf_document(file_path) for file_path in file_paths]\n",
    "# リストをフラット化\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1097, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# `docs_list` を Document 型に変換\n",
    "docs_list_converted = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in docs_list]\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list_converted)\n",
    "print(len(doc_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "\n",
    "# db = Chroma.from_documents(doc_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import tokenizer\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# 単語単位のn-gramを作成\n",
    "def generate_word_ngrams(text, i, j, binary=False):\n",
    "    tokenizer_obj = dictionary.Dictionary(dict=\"core\").create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.A\n",
    "    tokens = tokenizer_obj.tokenize(text, mode)\n",
    "    words = [token.surface() for token in tokens]\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(i, j + 1):\n",
    "        for k in range(len(words) - n + 1):\n",
    "            ngram = tuple(words[k : k + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    if binary:\n",
    "        ngrams = list(set(ngrams))  # 重複を削除\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def preprocess_word_func(text: str) -> List[str]:\n",
    "    return generate_word_ngrams(text, 1, 1, True)\n",
    "\n",
    "\n",
    "# 文字単位のn-gramを作成\n",
    "def generate_character_ngrams(text, i, j, binary=False):\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(i, j + 1):\n",
    "        for k in range(len(text) - n + 1):\n",
    "            ngram = text[k : k + n]\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    if binary:\n",
    "        ngrams = list(set(ngrams))  # 重複を削除\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def preprocess_char_func(text: str) -> List[str]:\n",
    "    i, j = 1, 3\n",
    "    if len(text) < i:\n",
    "        return [text]\n",
    "    return generate_character_ngrams(text, i, j, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語と文字のBM25Retrieverを作成\n",
    "word_retriever = BM25Retriever.from_documents(doc_splits, preprocess_func=preprocess_word_func)\n",
    "char_retriever = BM25Retriever.from_documents(doc_splits, preprocess_func=preprocess_char_func)\n",
    "word_retriever.k = 4\n",
    "char_retriever.k = 4\n",
    "\n",
    "# EnsembleRetrieverを作成\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[word_retriever, char_retriever], weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len = 4\n",
      "metadata = {'source': '../dataset/Hada_Labo_Gokujun_Lotion_Overview.pdf'}\n",
      "本製品の容器には、環境に配慮したバイオマス原料を⼀部使⽤しています。\n",
      "＊：加⽔分解ヒアルロン酸（ナノ化ヒアルロン酸）、アセチルヒアルロン酸Na（スーパーヒアルロン酸）、乳酸球菌／ヒアルロン酸発\n",
      "酵液（乳酸発酵ヒアルロン酸）、ヒアルロン酸Na\n",
      "◆本品は、航空法で定める航空危険物に該当しません。\n",
      "★販売名：ハダラボモイスト化粧⽔d\n",
      "使⽤上の注意\n",
      "＜相談すること＞\n",
      "○肌に異常が⽣じていないかよく注意して使⽤すること。使⽤中、⼜は使⽤後⽇光にあたって、⾚み、はれ、か\n",
      "ゆみ、刺激、⾊抜け（⽩斑等）や⿊ずみ等の異常が現れた時は、使⽤を中⽌し、⽪フ科専⾨医等へ相談するこ\n",
      "と。そのまま使⽤を続けると症状が悪化することがある。\n",
      "＜その他使⽤上の注意＞\n",
      "○傷、はれもの、湿疹等、異常のある部位には使⽤しないこと。\n",
      "○⽬に⼊らないように注意し、⼊った時はすぐに⽔⼜はぬるま湯で洗い流すこと。なお、異常が残る場合は、眼\n",
      "科医に相談すること。\n",
      "肌ラボ 極潤ヒアルロン液に関連する製品\n",
      "当社は、お客様のウェブ体験の向上のため、アクセスを分析しコンテンツや広告をパーソナライズするためにクッキーを使⽤し Cookie 設定\n",
      "ます。詳細はプライバシーポリシーをご確認ください。プライバシーポリシー\n",
      "すべての Cookie を受け⼊れる\n",
      "https://jp.rohto.com/hadalabo/gokujun-lotion/ 1/2\n"
     ]
    }
   ],
   "source": [
    "query = \"肌ラボ 極潤ヒアルロン液の使用上の注意点を教えてください。\"\n",
    "\n",
    "context_docs = ensemble_retriever.invoke(query)\n",
    "print(f\"len = {len(context_docs)}\")\n",
    "\n",
    "first_doc = context_docs[0]\n",
    "print(f\"metadata = {first_doc.metadata}\")\n",
    "print(first_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCELを使ったRAGのChainの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "'''\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肌ラボ 極潤ヒアルロン液の使用上の注意点は以下の通りです：\n",
      "\n",
      "1. **肌の異常に注意**: 使用中または使用後に日光にあたって、赤み、はれ、かゆみ、刺激、色抜け（白斑等）や黒ずみ等の異常が現れた場合は、使用を中止し、皮膚科専門医等に相談すること。\n",
      "\n",
      "2. **異常のある部位には使用しない**: 傷、はれもの、湿疹等、異常のある部位には使用しないこと。\n",
      "\n",
      "3. **目に入らないように注意**: 目に入った場合はすぐに水またはぬるま湯で洗い流し、異常が残る場合は眼科医に相談すること。\n",
      "\n",
      "これらの注意点を守って使用することが推奨されています。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    "\n",
    "query = \"肌ラボ 極潤ヒアルロン液の使用上の注意点を教えてください。\"\n",
    "\n",
    "output = chain.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragrohto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

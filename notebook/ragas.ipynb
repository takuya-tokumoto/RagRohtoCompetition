{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参照\n",
    "    - [RAGコンペ参加記 (raggle)](https://qiita.com/ctc-j-ikai/items/9980f6a1c11ef444ba4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas==0.1.14\n",
      "  Using cached ragas-0.1.14-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (1.26.4)\n",
      "Collecting datasets (from ragas==0.1.14)\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (0.8.0)\n",
      "Requirement already satisfied: langchain in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (0.2.12)\n",
      "Requirement already satisfied: langchain-core in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (0.2.29)\n",
      "Requirement already satisfied: langchain-community in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (0.2.11)\n",
      "Requirement already satisfied: langchain-openai in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (0.1.21)\n",
      "Requirement already satisfied: openai>1 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (1.58.1)\n",
      "Collecting pysbd>=0.3.4 (from ragas==0.1.14)\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from ragas==0.1.14) (1.6.0)\n",
      "Collecting appdirs (from ragas==0.1.14)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (2.10.4)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from openai>1->ragas==0.1.14) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (3.16.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->ragas==0.1.14)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas==0.1.14)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (2.32.3)\n",
      "Collecting xxhash (from datasets->ragas==0.1.14)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->ragas==0.1.14)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas==0.1.14)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (0.27.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from datasets->ragas==0.1.14) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langchain->ragas==0.1.14) (2.0.36)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langchain->ragas==0.1.14) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langchain->ragas==0.1.14) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langchain->ragas==0.1.14) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langchain-core->ragas==0.1.14) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langchain-community->ragas==0.1.14) (0.6.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from tiktoken->ragas==0.1.14) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from aiohttp->datasets->ragas==0.1.14) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>1->ragas==0.1.14) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.14) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.14) (0.9.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.1.14) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.1.14) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas==0.1.14) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas==0.1.14) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas==0.1.14) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas==0.1.14) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>1->ragas==0.1.14) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>1->ragas==0.1.14) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from requests>=2.32.2->datasets->ragas==0.1.14) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from requests>=2.32.2->datasets->ragas==0.1.14) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.1.14) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from pandas->datasets->ragas==0.1.14) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from pandas->datasets->ragas==0.1.14) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from pandas->datasets->ragas==0.1.14) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.1.14) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.14) (1.0.0)\n",
      "Using cached ragas-0.1.14-py3-none-any.whl (163 kB)\n",
      "Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: appdirs, xxhash, pysbd, pyarrow, fsspec, dill, multiprocess, datasets, ragas\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "Successfully installed appdirs-1.4.4 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pyarrow-18.1.0 pysbd-0.3.4 ragas-0.1.14 xxhash-3.5.0\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/ubuntu/miniconda3/envs/ragrohto/lib/python3.11/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install ragas==0.1.14\n",
    "# !pip install nest-asyncio==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Callable, Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# .envファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-rohto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オフライン評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_urls = [\n",
    "    \"/home/ubuntu/gitwork/RagRohtoCompetition/dataset/Financial_Statements_2023.pdf\",\n",
    "    \"/home/ubuntu/gitwork/RagRohtoCompetition/dataset/Hada_Labo_Gokujun_Lotion_Overview.pdf\",\n",
    "    \"/home/ubuntu/gitwork/RagRohtoCompetition/dataset/Shibata_et_al_Research_Article.pdf\",\n",
    "    \"/home/ubuntu/gitwork/RagRohtoCompetition/dataset/V_Rohto_Premium_Product_Information.pdf\",\n",
    "    \"/home/ubuntu/gitwork/RagRohtoCompetition/dataset/Well-Being_Report_2024.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "\n",
    "def load_pdf_document(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"PDFドキュメントを読み込み、各ページのテキストを抽出\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 読み込むPDFファイルのパス。\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: 各ページのテキストとメタデータを含む辞書のリスト。\n",
    "            - \"content\" (str): ページから抽出されたテキスト内容。\n",
    "            - \"metadata\" (Dict[str, str]): メタデータ情報（以下を含む）:\n",
    "                - \"source\" (str): PDFファイルのパス。\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            documents.append({\"content\": page.extract_text(), \"metadata\": {\"source\": file_path}})\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def document_loader(file_paths: List[str], loader_func) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    複数のPDFファイルパスを受け取り、各ファイルから抽出されたドキュメントを\n",
    "    フラットなリストとして返す。\n",
    "\n",
    "    Args:\n",
    "        file_paths (List[str]): PDFファイルのパスリスト。\n",
    "        loader_func (Callable): 単一のPDFファイルを読み込む関数。\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: 全ファイルのドキュメントをフラットなリストとして返す。\n",
    "    \"\"\"\n",
    "    # PDFファイルを読み込む\n",
    "    docs = [loader_func(file_path) for file_path in file_paths]\n",
    "    # リストをフラット化\n",
    "    all_documents = [item for sublist in docs for item in sublist]\n",
    "    return all_documents\n",
    "\n",
    "\n",
    "def document_transformer(\n",
    "    docs_list: List[dict], chunk_size: int = 1100, chunk_overlap: int = 100, separator: str = \"\\n\"\n",
    ") -> List[Document]:\n",
    "    \"\"\"ドキュメントリストを LangChain の Document 型に変換し、指定されたサイズで分割する。\n",
    "\n",
    "    Args:\n",
    "        docs_list (List[dict]): `content` と `metadata` を持つドキュメントのリスト。\n",
    "        chunk_size (int): 分割時のチャンクサイズ。\n",
    "        chunk_overlap (int): 分割時のチャンクのオーバーラップサイズ。\n",
    "        separator (str): チャンク間の区切り文字。\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: 分割後の LangChain の Document 型のリスト。\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to Document type\n",
    "    docs_list_converted = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in docs_list]\n",
    "\n",
    "    # Step 2: Initialize text splitter\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=separator,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "\n",
    "    # Step 3: Split documents\n",
    "    doc_splits = text_splitter.split_documents(docs_list_converted)\n",
    "\n",
    "    return doc_splits, docs_list_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = document_loader(pdf_file_urls, load_pdf_document)\n",
    "doc_splits, docs_list_converted = document_transformer(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                   \n",
      "Generating: 100%|██████████| 4/4 [00:39<00:00,  9.76s/it]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    critic_llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    docs_list_converted,\n",
    "    test_size=4,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the health benefits associated with t...</td>\n",
       "      <td>[「Demas茶」を新発売。日常の食習慣と健康の接点を増やし、機能性素材の\\nる「」うごく」...</td>\n",
       "      <td>The context does not provide specific health b...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>軽度疾患における当社の取り組みは何ですか？</td>\n",
       "      <td>[当社が取り組む事業領域は、健康、未病、軽度疾患、病気の全てのステージにおける美と健康の提供...</td>\n",
       "      <td>Answer is not present in the context</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does 'Go Ethical' relate to reducing envir...</td>\n",
       "      <td>[ロートの 価値創造 事業を通じた 人的資本の 持続可能な コーポレート・\\nロートの今 社...</td>\n",
       "      <td>The context mentions that 'Go Ethical' is aime...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does EDINET ensure accurate corporate disc...</td>\n",
       "      <td>[94/134\\nEDINET提出書類\\nロート製薬株式会社(E00942)\\n有価証券報告...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the health benefits associated with t...   \n",
       "1                              軽度疾患における当社の取り組みは何ですか？   \n",
       "2  How does 'Go Ethical' relate to reducing envir...   \n",
       "3  How does EDINET ensure accurate corporate disc...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [「Demas茶」を新発売。日常の食習慣と健康の接点を増やし、機能性素材の\\nる「」うごく」...   \n",
       "1  [当社が取り組む事業領域は、健康、未病、軽度疾患、病気の全てのステージにおける美と健康の提供...   \n",
       "2  [ロートの 価値創造 事業を通じた 人的資本の 持続可能な コーポレート・\\nロートの今 社...   \n",
       "3  [94/134\\nEDINET提出書類\\nロート製薬株式会社(E00942)\\n有価証券報告...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The context does not provide specific health b...         simple   \n",
       "1               Answer is not present in the context         simple   \n",
       "2  The context mentions that 'Go Ethical' is aime...      reasoning   \n",
       "3  The answer to given question is not present in...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  \n",
       "1  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  \n",
       "2  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  \n",
       "3  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role do OTC pharmaceuticals play in the h...</td>\n",
       "      <td>[のアイケアに関する知見を掛け合わせ、眼を基点とした人\\nや社会のWell-beingの実現...</td>\n",
       "      <td>OTC pharmaceuticals play a role in the healthc...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>持続的成長とは何ですか？</td>\n",
       "      <td>[締役会\\nすべての取締役で組成され、出席義務のある監査役の出席のもと運営されています。取締...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What boosts employee well-being in a supportiv...</td>\n",
       "      <td>[�社は、多様な価値観を持つ自律した個人\\nが、自己成長のために学び続ける意思を持ち続けられ...</td>\n",
       "      <td>The context discusses various factors that can...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's essential for sustainable biz growth?</td>\n",
       "      <td>[バナンス強化のための課題や 応、設備への投資配分も確り行っております。このような投 0\\n...</td>\n",
       "      <td>The context does not provide a specific answer...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '/home/ubuntu/gitwork/RagRohtoComp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What role do OTC pharmaceuticals play in the h...   \n",
       "1                                       持続的成長とは何ですか？   \n",
       "2  What boosts employee well-being in a supportiv...   \n",
       "3       What's essential for sustainable biz growth?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [のアイケアに関する知見を掛け合わせ、眼を基点とした人\\nや社会のWell-beingの実現...   \n",
       "1  [締役会\\nすべての取締役で組成され、出席義務のある監査役の出席のもと運営されています。取締...   \n",
       "2  [�社は、多様な価値観を持つ自律した個人\\nが、自己成長のために学び続ける意思を持ち続けられ...   \n",
       "3  [バナンス強化のための課題や 応、設備への投資配分も確り行っております。このような投 0\\n...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  OTC pharmaceuticals play a role in the healthc...         simple   \n",
       "1  The answer to given question is not present in...         simple   \n",
       "2  The context discusses various factors that can...      reasoning   \n",
       "3  The context does not provide a specific answer...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  \n",
       "1  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  \n",
       "2  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  \n",
       "3  [{'source': '/home/ubuntu/gitwork/RagRohtoComp...          True  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas().to_csv(\"test_dt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pdfplumber\n",
    "\n",
    "\n",
    "def load_pdf_document(file_path):\n",
    "    documents = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            documents.append({\"content\": page.extract_text(), \"metadata\": {\"source\": file_path}})\n",
    "    return documents\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    \"../dataset/Financial_Statements_2023.pdf\",\n",
    "    \"../dataset/Hada_Labo_Gokujun_Lotion_Overview.pdf\",\n",
    "    \"../dataset/Shibata_et_al_Research_Article.pdf\",\n",
    "    \"../dataset/V_Rohto_Premium_Product_Information.pdf\",\n",
    "    \"../dataset/Well-Being_Report_2024.pdf\",\n",
    "]\n",
    "\n",
    "# PDFファイルを読み込む\n",
    "docs = [load_pdf_document(file_path) for file_path in file_paths]\n",
    "# リストをフラット化\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1097, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# `docs_list` を Document 型に変換\n",
    "docs_list_converted = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in docs_list]\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list_converted)\n",
    "print(len(doc_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "\n",
    "# db = Chroma.from_documents(doc_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import tokenizer\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# 単語単位のn-gramを作成\n",
    "def generate_word_ngrams(text, i, j, binary=False):\n",
    "    tokenizer_obj = dictionary.Dictionary(dict=\"core\").create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.A\n",
    "    tokens = tokenizer_obj.tokenize(text, mode)\n",
    "    words = [token.surface() for token in tokens]\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(i, j + 1):\n",
    "        for k in range(len(words) - n + 1):\n",
    "            ngram = tuple(words[k : k + n])\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    if binary:\n",
    "        ngrams = list(set(ngrams))  # 重複を削除\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def preprocess_word_func(text: str) -> List[str]:\n",
    "    return generate_word_ngrams(text, 1, 1, True)\n",
    "\n",
    "\n",
    "# 文字単位のn-gramを作成\n",
    "def generate_character_ngrams(text, i, j, binary=False):\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(i, j + 1):\n",
    "        for k in range(len(text) - n + 1):\n",
    "            ngram = text[k : k + n]\n",
    "            ngrams.append(ngram)\n",
    "\n",
    "    if binary:\n",
    "        ngrams = list(set(ngrams))  # 重複を削除\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def preprocess_char_func(text: str) -> List[str]:\n",
    "    i, j = 1, 3\n",
    "    if len(text) < i:\n",
    "        return [text]\n",
    "    return generate_character_ngrams(text, i, j, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語と文字のBM25Retrieverを作成\n",
    "word_retriever = BM25Retriever.from_documents(doc_splits, preprocess_func=preprocess_word_func)\n",
    "char_retriever = BM25Retriever.from_documents(doc_splits, preprocess_func=preprocess_char_func)\n",
    "word_retriever.k = 4\n",
    "char_retriever.k = 4\n",
    "\n",
    "# EnsembleRetrieverを作成\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[word_retriever, char_retriever], weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len = 4\n",
      "metadata = {'source': '../dataset/Hada_Labo_Gokujun_Lotion_Overview.pdf'}\n",
      "本製品の容器には、環境に配慮したバイオマス原料を⼀部使⽤しています。\n",
      "＊：加⽔分解ヒアルロン酸（ナノ化ヒアルロン酸）、アセチルヒアルロン酸Na（スーパーヒアルロン酸）、乳酸球菌／ヒアルロン酸発\n",
      "酵液（乳酸発酵ヒアルロン酸）、ヒアルロン酸Na\n",
      "◆本品は、航空法で定める航空危険物に該当しません。\n",
      "★販売名：ハダラボモイスト化粧⽔d\n",
      "使⽤上の注意\n",
      "＜相談すること＞\n",
      "○肌に異常が⽣じていないかよく注意して使⽤すること。使⽤中、⼜は使⽤後⽇光にあたって、⾚み、はれ、か\n",
      "ゆみ、刺激、⾊抜け（⽩斑等）や⿊ずみ等の異常が現れた時は、使⽤を中⽌し、⽪フ科専⾨医等へ相談するこ\n",
      "と。そのまま使⽤を続けると症状が悪化することがある。\n",
      "＜その他使⽤上の注意＞\n",
      "○傷、はれもの、湿疹等、異常のある部位には使⽤しないこと。\n",
      "○⽬に⼊らないように注意し、⼊った時はすぐに⽔⼜はぬるま湯で洗い流すこと。なお、異常が残る場合は、眼\n",
      "科医に相談すること。\n",
      "肌ラボ 極潤ヒアルロン液に関連する製品\n",
      "当社は、お客様のウェブ体験の向上のため、アクセスを分析しコンテンツや広告をパーソナライズするためにクッキーを使⽤し Cookie 設定\n",
      "ます。詳細はプライバシーポリシーをご確認ください。プライバシーポリシー\n",
      "すべての Cookie を受け⼊れる\n",
      "https://jp.rohto.com/hadalabo/gokujun-lotion/ 1/2\n"
     ]
    }
   ],
   "source": [
    "query = \"肌ラボ 極潤ヒアルロン液の使用上の注意点を教えてください。\"\n",
    "\n",
    "context_docs = ensemble_retriever.invoke(query)\n",
    "print(f\"len = {len(context_docs)}\")\n",
    "\n",
    "first_doc = context_docs[0]\n",
    "print(f\"metadata = {first_doc.metadata}\")\n",
    "print(first_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCELを使ったRAGのChainの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "'''\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肌ラボ 極潤ヒアルロン液の使用上の注意点は以下の通りです：\n",
      "\n",
      "1. **肌の異常に注意**: 使用中または使用後に日光にあたって、赤み、はれ、かゆみ、刺激、色抜け（白斑等）や黒ずみ等の異常が現れた場合は、使用を中止し、皮膚科専門医等に相談すること。\n",
      "\n",
      "2. **異常のある部位には使用しない**: 傷、はれもの、湿疹等、異常のある部位には使用しないこと。\n",
      "\n",
      "3. **目に入らないように注意**: 目に入った場合はすぐに水またはぬるま湯で洗い流し、異常が残る場合は眼科医に相談すること。\n",
      "\n",
      "これらの注意点を守って使用することが推奨されています。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    "\n",
    "query = \"肌ラボ 極潤ヒアルロン液の使用上の注意点を教えてください。\"\n",
    "\n",
    "output = chain.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragrohto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
